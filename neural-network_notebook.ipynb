{"cells":[{"cell_type":"code","execution_count":33,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-11T17:47:11.821414Z","iopub.status.busy":"2024-08-11T17:47:11.820913Z","iopub.status.idle":"2024-08-11T17:47:11.830446Z","shell.execute_reply":"2024-08-11T17:47:11.829395Z","shell.execute_reply.started":"2024-08-11T17:47:11.821372Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/digit-recognizer/sample_submission.csv\n","/kaggle/input/digit-recognizer/train.csv\n","/kaggle/input/digit-recognizer/test.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T17:47:11.832367Z","iopub.status.busy":"2024-08-11T17:47:11.832029Z","iopub.status.idle":"2024-08-11T17:47:11.846220Z","shell.execute_reply":"2024-08-11T17:47:11.845005Z","shell.execute_reply.started":"2024-08-11T17:47:11.832321Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import pickle"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T17:47:11.848987Z","iopub.status.busy":"2024-08-11T17:47:11.847895Z","iopub.status.idle":"2024-08-11T17:47:11.859941Z","shell.execute_reply":"2024-08-11T17:47:11.858422Z","shell.execute_reply.started":"2024-08-11T17:47:11.848944Z"},"trusted":true},"outputs":[],"source":["class Linear:\n","    # initialisng weight and biases\n","    def __init__(self, in_features, out_features):\n","        self.weights = np.random.randn(in_features, out_features) * 0.01\n","        self.biases = np.zeros((1, out_features))\n","        self.input = None\n","        self.grad_weights = None\n","        self.grad_biases = None\n","\n","    def forward(self, x): # for forward propogation\n","        self.input = x\n","        return np.dot(x, self.weights) + self.biases\n","\n","    def backward(self, d_out): # for backward propogation\n","        self.grad_weights = np.dot(self.input.T, d_out)\n","        self.grad_biases = np.sum(d_out, axis=0, keepdims=True)\n","        return np.dot(d_out, self.weights.T)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T17:47:11.862431Z","iopub.status.busy":"2024-08-11T17:47:11.861599Z","iopub.status.idle":"2024-08-11T17:47:11.871847Z","shell.execute_reply":"2024-08-11T17:47:11.870447Z","shell.execute_reply.started":"2024-08-11T17:47:11.862385Z"},"trusted":true},"outputs":[],"source":["# Activation classes\n","\n","class ReLU: #Rectified Linear Unit activation\n","    def forward(self, x):\n","        self.input = x\n","        return np.maximum(0, x)\n","\n","    def backward(self, d_out): # gradients for ReLU activation\n","        d_input = d_out.copy()\n","        d_input[self.input <= 0] = 0\n","        return d_input\n","\n","class Softmax: #softmax activation\n","    def forward(self, x):\n","        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n","        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n","        return self.output\n","\n","    def backward(self, d_out): # gradients for softmax activation\n","        return d_out\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T17:47:11.876120Z","iopub.status.busy":"2024-08-11T17:47:11.875250Z","iopub.status.idle":"2024-08-11T17:47:11.885547Z","shell.execute_reply":"2024-08-11T17:47:11.884414Z","shell.execute_reply.started":"2024-08-11T17:47:11.876076Z"},"trusted":true},"outputs":[],"source":["class CrossEntropyLoss: # calculating cross entropy loss\n","    def forward(self, y_pred, y_true):\n","        self.y_pred = y_pred\n","        self.y_true = y_true\n","        self.n = y_true.shape[0]\n","        return -np.sum(y_true * np.log(y_pred + 1e-10)) / self.n\n","\n","    def backward(self): # computing gradient of cross entropy loss\n","        return (self.y_pred - self.y_true) / self.n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T17:47:11.887651Z","iopub.status.busy":"2024-08-11T17:47:11.886936Z","iopub.status.idle":"2024-08-11T17:47:11.897600Z","shell.execute_reply":"2024-08-11T17:47:11.896598Z","shell.execute_reply.started":"2024-08-11T17:47:11.887613Z"},"trusted":true},"outputs":[],"source":["# Stochastic Gradient Descent \n","class SGD:\n","    def __init__(self, learning_rate=0.01):\n","        self.learning_rate = learning_rate\n","\n","    def step(self, layer):\n","        if hasattr(layer, 'grad_weights'):\n","            layer.weights -= self.learning_rate * layer.grad_weights\n","            layer.biases -= self.learning_rate * layer.grad_biases"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T17:47:11.900179Z","iopub.status.busy":"2024-08-11T17:47:11.899789Z","iopub.status.idle":"2024-08-11T17:47:11.921745Z","shell.execute_reply":"2024-08-11T17:47:11.920213Z","shell.execute_reply.started":"2024-08-11T17:47:11.900145Z"},"trusted":true},"outputs":[],"source":["# Neural Network Model\n","class Model:\n","    \n","    def __init__(self): # initialising model\n","        self.layers = []\n","        self.loss_fn = None\n","        self.optimizer = None\n","\n","    def add(self, layer): # adding layer to the neural network\n","        self.layers.append(layer)\n","\n","    def compile(self, loss_fn, optimizer): # setting up loss and optimizer function\n","        self.loss_fn = loss_fn\n","        self.optimizer = optimizer\n","\n","    def forward(self, x): # forward pass\n","        for layer in self.layers:\n","            x = layer.forward(x)\n","        return x\n","\n","    def backward(self, loss_grad): # backward pass\n","        for layer in reversed(self.layers):\n","            loss_grad = layer.backward(loss_grad)\n","\n","    def train(self, x_train, y_train, epochs, batch_size): #TRAINING THE MODEL\n","\n","        for epoch in range(epochs):\n","            indices = np.arange(x_train.shape[0])\n","            #np.random.shuffle(indices)\n","            x_train, y_train = x_train[indices], y_train[indices]\n","            \n","            for start in range(0, x_train.shape[0], batch_size):\n","                end = min(start + batch_size, x_train.shape[0])\n","                x_batch, y_batch = x_train[start:end], y_train[start:end]\n","                \n","                predictions = self.forward(x_batch)\n","                loss = self.loss_fn.forward(predictions, y_batch)\n","                loss_grad = self.loss_fn.backward()\n","                self.backward(loss_grad)\n","                \n","                for layer in self.layers:\n","                    self.optimizer.step(layer)\n","                \n","            print(f'EPOCH {epoch + 1}/{epochs}, LOSS: {loss}')\n","\n","    def evaluate(self, x_test, y_test): # Evaluation of the model\n","        predictions = self.forward(x_test)\n","        loss = self.loss_fn.forward(predictions, y_test)\n","        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n","        return loss, accuracy\n","    \n","    def save_weights(self, filename):\n","        # Save the weights and biases of the model to a file\n","        weights_dict = {}\n","        for i, layer in enumerate(self.layers):\n","            if hasattr(layer, 'weights'):\n","                weights_dict[f'layer_{i}_weights'] = layer.weights\n","                weights_dict[f'layer_{i}_biases'] = layer.biases\n","        \n","        with open(filename, 'wb') as f:\n","            pickle.dump(weights_dict, f)\n","        print(f\"Model weights saved to {filename}\")\n","\n","    def load_weights(self, filename):\n","        # Loading the weights and biases from a file into the model\n","        with open(filename, 'rb') as f:\n","            weights_dict = pickle.load(f)\n","        \n","        for i, layer in enumerate(self.layers):\n","            if hasattr(layer, 'weights'):\n","                layer.weights = weights_dict[f'layer_{i}_weights']\n","                layer.biases = weights_dict[f'layer_{i}_biases']\n","        print(f\"Model weights loaded from {filename}\")"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T17:47:11.924530Z","iopub.status.busy":"2024-08-11T17:47:11.924011Z","iopub.status.idle":"2024-08-11T17:47:40.212581Z","shell.execute_reply":"2024-08-11T17:47:40.211497Z","shell.execute_reply.started":"2024-08-11T17:47:11.924488Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH 1/20, LOSS: 1.428813159730113\n","EPOCH 2/20, LOSS: 0.5450430090265324\n","EPOCH 3/20, LOSS: 0.33174064850649326\n","EPOCH 4/20, LOSS: 0.2251559227225986\n","EPOCH 5/20, LOSS: 0.1660131002445489\n","EPOCH 6/20, LOSS: 0.12987812962966078\n","EPOCH 7/20, LOSS: 0.10587775656422505\n","EPOCH 8/20, LOSS: 0.08958536692777577\n","EPOCH 9/20, LOSS: 0.07785262203541149\n","EPOCH 10/20, LOSS: 0.0691116819314456\n","EPOCH 11/20, LOSS: 0.062434585860398156\n","EPOCH 12/20, LOSS: 0.05684364824083592\n","EPOCH 13/20, LOSS: 0.052093844696240614\n","EPOCH 14/20, LOSS: 0.048194298867363566\n","EPOCH 15/20, LOSS: 0.04504730712678991\n","EPOCH 16/20, LOSS: 0.04250104630567056\n","EPOCH 17/20, LOSS: 0.04015491065068518\n","EPOCH 18/20, LOSS: 0.038192599211573655\n","EPOCH 19/20, LOSS: 0.03639418585936896\n","EPOCH 20/20, LOSS: 0.03495184577546863\n","Test Loss: 0.2371432049869906, Test Accuracy: 0.9319285714285714\n"]}],"source":["\n","# Load and preprocess data\n","train_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n","x = train_df.drop(columns='label').values / 255.0\n","y = pd.get_dummies(train_df['label']).values\n","\n","# Define a simple neural network using the framework\n","model = Model()\n","model.add(Linear(784, 128))\n","model.add(ReLU())\n","model.add(Linear(128, 10))\n","model.add(Softmax())\n","\n","# Compile the model with loss and optimizer\n","loss = CrossEntropyLoss()\n","optimizer = SGD(learning_rate=0.01)\n","model.compile(loss, optimizer)\n","\n","# Train the model\n","model.train(x, y, epochs=20, batch_size=64)\n","\n","# LOAD OR SAVE WEIGHTS HERE\n","\n","# Evaluate the model\n","test_loss, test_accuracy = model.evaluate(x, y)\n","print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":861823,"sourceId":3004,"sourceType":"competition"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
